<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="机器学习之Logistic回归" />
<meta property="og:description" content="分类是监督学习中的一个重要问题，当输出变量Y取有限个离散值时，预测问题便成为分类问题。此时的X可以是离散的或者连续的。

" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://maodanp.github.io/2017/05/26/logistic-regression/" />



<meta property="article:published_time" content="2017-05-26T00:00:00&#43;00:00"/>

<meta property="article:modified_time" content="2017-05-26T00:00:00&#43;00:00"/>











<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="机器学习之Logistic回归"/>
<meta name="twitter:description" content="分类是监督学习中的一个重要问题，当输出变量Y取有限个离散值时，预测问题便成为分类问题。此时的X可以是离散的或者连续的。

"/>


    <link rel="canonical" href="http://maodanp.github.io/2017/05/26/logistic-regression/">

    <title>
      
        机器学习之Logistic回归 | Danping&#39;s Blog
      
    </title>

    
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">

    <link href="http://maodanp.github.io/css/style.css" rel="stylesheet">

    

    

    
  </head>
  <body>
    
      <header class="blog-header">
    <nav class="navbar navbar-expand-md navbar-light bg-light">
        <a class="navbar-brand" href="/">
            Danping&#39;s Blog
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false"
            aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse justify-content-between" id="navbarNav">
            <ul class="navbar-nav">
                
                
                <li class="nav-item ">
                    <a class="nav-link" href="/">主页</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="/categories/%e6%8a%80%e6%9c%af%e5%bf%97/">技术志</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="/categories/%E5%B7%A5%E5%85%B7%E7%AE%B1/">工具箱</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="/categories/%e6%9d%82%e8%b0%88%e9%9b%86/">杂谈集</a>
                </li>
                
                <li class="nav-item ">
                    <a class="nav-link" href="/about/">关于</a>
                </li>
                
            </ul>
            
        </div>
	<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    displayMath: [['$$','$$']],
    processEscapes: true,
    processEnvironments: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
    TeX: { equationNumbers: { autoNumber: "AMS" },
         extensions: ["AMSmath.js", "AMSsymbols.js"] }
  }
  });
  MathJax.Hub.Queue(function() {
    
    
    
    var all = MathJax.Hub.getAllJax(), i;
    for(i = 0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });

  MathJax.Hub.Config({
  
  TeX: { equationNumbers: { autoNumber: "AMS" } }
  });
</script>
    </nav>
</header>

    

    
    <div class="container">
      <div class="row">
        <div class="col-12 col-lg-8 blog-main">

          

<article class="blog-post">
    <header>
        <h2 class="blog-post-title">
            <a class="text-dark" href="/2017/05/26/logistic-regression/">机器学习之Logistic回归</a>
        </h2>
        


<div class="blog-post-date text-secondary">
    
        May 26, 2017
    
    
        by Danping Mao
    
</div>
        
<div class="blog-post-tags text-secondary">
    <strong>Tags:</strong>
    
        <a class="badge badge-primary" href="/tags/machine-learning">machine learning</a>
    
        <a class="badge badge-primary" href="/tags/logistic%E5%9B%9E%E5%BD%92">Logistic回归</a>
    
</div>

        
<div class="blog-post-categories text-secondary">
    <strong>Categories:</strong>
    
        <a class="badge badge-primary" href="/categories/%E6%8A%80%E6%9C%AF%E5%BF%97">技术志</a>
    
</div>

    </header>
    <hr>
    <p>分类是监督学习中的一个重要问题，当输出变量Y取有限个离散值时，预测问题便成为分类问题。此时的X可以是离散的或者连续的。</p>

<p></p>

<h2 id="logistic回归">Logistic回归</h2>

<h3 id="背景与定义">背景与定义</h3>

<p>分类问题包括学习和分类两个过程。在学习过程中，根据已知的训练数据集利用有效的学习方法学习一个分类器；在分类过程中，利用学习的分类器对新的输入实例进行分类。</p>

<p>评价分类器性能的指标一般是分类准确率,即对于给定的测试数据集，分类器正确分类的样本数与总样本数之比。</p>

<pre><code>-------------------------------------------------------
|                   | Actual class 1 | Actual class 0 |
| ----------------- | -------------- | -------------- |
| Predicted class 1 |        TP      |       FP       |
| Predicted class 0 |        FN      |       TN       |
-------------------------------------------------------
</code></pre>

<p>精确率定义为：
<code>$$
P = \frac{TP}{TP+FP}
$$</code>
召回率定义为：
<code>$$
R = \frac{TP}{TP+FN}
$$</code>
此外还有$F_1$值，是精确率与召回率的调和均值。
<code>$$
\frac{2}{F_1} = \frac{1}{P}+\frac{1}{R} \\
F_1 = \frac{2TP}{2TP+FP+FN}
$$</code></p>

<p>在Logistic regression二分类问题中，我们可以使用sigmoid函数将输入$Wx+b$映射到$(0,1)$区间中，从而得到属于某个类别的概率。将这个问题进行泛化，推广到多分类问题中，可以使用<strong>softmax</strong>函数，对输出的值归一化为概率值。</p>

<h4 id="交叉熵">交叉熵</h4>

<p>说交叉熵之前先介绍相对熵，相对熵又称为KL散度（Kullback-Leibler Divergence），用来衡量两个分布之间的距离，记为$D_{KL}(p||q)$
<code>$$
\begin{align}
D_{KL}(p||q)=
\sum_{x\in X}p(x)log\frac{p(x)}{q(x)} \\ 
=\sum_{x\in X}p(x)logp(x)-\sum_{x\in X}p(x)logq(x) \\ 
=-H(p) - \sum_{x\in X}p(x)logq(x)
\end{align}
$$</code>
其中$H(p)$是$p$的熵。</p>

<p>假设有两个$p$和$q$，它们在给定样本集上的交叉熵定义为：
<code>$$
CE(p,q)=-\sum_{x\in X}p(x)log q(x) = H(p) +D_{KL}(p||q)
$$</code>
从这里可以看出，交叉熵和相对熵相差了$H(p)$，而当$p$已知的时候，$H(p)$是个常数，所以交叉熵和相对熵在这里是等价的，反映了分布$p$和$q$之间的相似程度</p>

<p>对于一个样本来说，真是类标签分布与模型预测的类标签分布可以用交叉熵来表示：
<code>$$
l_{CE}=-\sum_{i=1}^Ct_ilog(y_i)
$$</code>
该等式与对数似然函数形式一样.</p>

<p>最终，对于所有的样本，有以下损失函数：
<code>$$
L = -\sum_{k=1}^n\sum_{i=1}^Ct_{ki}log(y_{ki})
$$</code>
其中，<code>$t_{ki}$</code>是样本$k$属于类别$i$的概率, $y_{ki}$是模型对样本$k$预测为属于类别$i$的概率。</p>

<h3 id="logistic回归-1">Logistic回归</h3>

<ul>
<li>Logistic/sigmoid 函数</li>
</ul>

<p><code>$$
h_{\theta}(x) = g(\theta^Tx) = \frac{1}{1+e^{-\theta^Tx}} \\
g'(x) = (\frac{1}{1+e^{-x}})'=\cdots=g(x)\cdot(1-g(x))
$$</code></p>

<ul>
<li>Logistic回归参数估计</li>
</ul>

<p>假定:
<code>$$
P(y=1|x;\theta) = h_{\theta}(x) \\
P(y=0|x;\theta) = 1- h_{\theta}(x) \\
P(y|x;\theta) = (h_{\theta}(x))^y (1-h_{\theta}(x))^{1-y}
$$</code>
对数似然函数:
<code>$$
\begin{align}
L(\theta) 
&amp;= \prod_{i=1}^mp(y^{(i)}|x^{(i)};\theta) \\
&amp;= \prod_{i=1}^m(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}} 
\end{align}
$$</code></p>

<p><code>$$
l(\theta) = lnL(\theta) = \sum_{i=1}^my^{(i)}logh(x^{(i)})+(1-y^{(i)})log(1-h(x^{(i)}))
$$</code></p>

<p><code>$$
\begin{align}
\frac{\partial l(\theta)}{\partial \theta_j} 
&amp;= \sum_{i=1}^m[\frac{y^{(i)}}{h(x^{(i)})} - \frac{1-y^{(i)}}{1-h(x^{(i)})}]\cdot\frac{\partial h(x^{(i)})}{\partial \theta_j} \\
&amp;= \sum_{i=1}^m[\frac{y^{(i)}}{h(x^{(i)})} - \frac{1-y^{(i)}}{1-h(x^{(i)})}]\cdot\frac{\partial g(\theta^Tx^{(i)})}{\partial \theta_j} \\
&amp;= \sum_{i=1}^m[\frac{y^{(i)}}{h(x^{(i)})} - \frac{1-y^{(i)}}{1-h(x^{(i)})}]\cdot h(x^{(i)}) \cdot (1-h(x^{(i)}))\frac{\partial \theta^Tx^{(i)}}{\partial \theta_j} \\
&amp;=\sum_{i=1}^m(y^{(i)}(1-g(\theta^Tx^{(i)}))-(1-y^{(i)})g(\theta^Tx^{(i)}))\cdot x_j^{(i)} \\
&amp;=\sum_{i=1}^m(y^{(i)} - g(\theta^Tx^{(i)}))\cdot x_j^{(i)}
\end{align}
$$</code></p>

<ul>
<li>Logistic交叉熵损失函数</li>
</ul>

<p>交叉熵损失函数公式：
<code>$$
J(\theta)=-\frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log(h_\theta(x^{(i)}))+(1-y^{(i)})\log(1-h_\theta(x^{(i)})),
$$</code>
以及$J(\theta)$的参数$\theta$的偏导数（用于诸如梯度下降法等优化算法的参数更新），如下：
<code>$$
\frac{\partial}{\partial\theta_{j}}J(\theta) =\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}
$$</code></p>

<ul>
<li><strong>logistic回归（是非问题）</strong>中，$y^{(i)}$取0或者1；</li>
<li><strong>softmax回归（多分类问题）</strong>中，$y^{(i)}$取1,2…k中的一个表示分类标号的一个数。</li>
</ul>

<h3 id="softmax回归">Softmax回归</h3>

<ul>
<li>softmax函数定义</li>
</ul>

<p>一直模型输出有$C$个值，其中$C$是要预测的类别数，模型可以是全连接网络的输出$a$, 即输出为$a_1, a_2,…,a<em>C$。对于每个样本，它属于类别$i$的概率为：
<code>$$
y_i = \frac{e^{a_i}}{\sum_{i=1}^Ce^{a_k}} \quad \forall i \in 1...C
$$</code>
上式中$\sum</em>{i=1}^Cy_i=1$ 保证了属于各个类别的概率和为1</p>

<ul>
<li>softmax函数求导</li>
</ul>

<p>对softmax函数进行求导，即求$\frac{\partial y_i}{\partial a_j}$</p>

<p>$e^{a_i}$分两对$a_j$求导分情况讨论：</p>

<ol>
<li>如果$i=j$，则求导结果为$e^{a_i}$</li>
<li>如果$i \ne j$，则求导结果为0</li>
</ol>

<p>当$i=j$时：
<code>$$
\frac{\partial y_i}{\partial a_j}=\frac{e^{a_i}\sum - e^{a_i}e^{a_j}}{\sum^2}=\frac{e^{a_i}}{\sum}\frac{\sum-e^{a_j}}{\sum}=y_i(1-y_j)
$$</code>
当$i \ne j$时：
<code>$$
\frac{\partial y_i}{\partial a_j}=\frac{0-e^{a_i}e^{a_j}}{\sum^2}=-\frac{e^{a_i}}{\sum}\frac{e^{a_j}}{\sum}=-y_iy_j
$$</code></p>

<ul>
<li>交叉熵损失函数求导</li>
</ul>

<p>对单个样本来说，loss function对输入$a_j$的导数为：
<code>$$
\frac{\partial l_{CE}}{\partial a_j}= -\sum_{i=1}^C\frac{\partial t_ilog(y_i)}{\partial a_j} =  -\sum_{i=1}^Ct_i\frac{1}{y_i}\frac{\partial y_i}{\partial a_j}
$$</code>
将求导结果带入：
<code>$$
-\sum_{i=1}^Ct_i\frac{1}{y_i}\frac{\partial y_i}{\partial a_j}=-\frac{t_i}{y_i}\frac{\partial y_i}{\partial a_i}-\sum{}_{i \ne j}^C\frac{t_i}{y_i}\frac{\partial y_i}{\partial a_j} \\ 
=-t_j+t_jy_j+\sum_{i \ne j}^Ct_iy_j = -t_j + \sum_{i =1}^Ct_iy_j \\
=-t_j + y_i\sum_{i=1}^Ct_i = y_j - t_j
$$</code></p>

<ul>
<li>梯度下降法表示</li>
</ul>

<p>与线性回归的参数估计一样，可以将批量梯度下降法表示如下：
<code>$$
\begin{align}
Repeat \quad until\quad  convergence \{ \\
&amp; \theta_j := \theta_j + \alpha\sum_{i=1}^m(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}\\
&amp;\}
\end{align}
$$</code>
随机梯度下降法表示如下：
<code>$$
\begin{align}
Repeat\{ \\
 &amp;for \quad i=1 \quad to \quad m, \{ \\
&amp;   \theta_j := \theta_j + \alpha (y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)} \\
&amp;\} \\
\}
\end{align}
$$</code>
其中:
<code>$$
h_\theta(x)=
\begin{cases}
\theta^Tx, &amp;\text{linear regression} \\[2ex]
\frac{1}{1+e^{-\theta^Tx}}, &amp;\text{logistic regression}
\end{cases}
$$</code></p>

<h2 id="参考阅读">参考阅读</h2>

<p><a href="http://blog.csdn.net/jasonzzj/article/details/52017438">交叉熵代价函数及其推导</a></p>

<p><a href="https://zhuanlan.zhihu.com/p/27223959">Softmax函数与交叉熵</a></p>

    
    

    <h4>See also</h4>
    <ul>
        
            <li><a href="/2017/05/20/linear-regression/">机器学习之线性回归</a></li>
        
            <li><a href="/2017/04/10/probability/">机器学习之概率论</a></li>
        
            <li><a href="/2017/04/03/math-analysis/">机器学习之数学分析</a></li>
        
    </ul>


</article>



        </div>

        <aside class="col-12 col-lg-3 ml-auto blog-sidebar">
    
        


<section>
    <h4>Recent Posts</h4>
    <ol class="list-unstyled">
        
        <li>
            <a href="/2017/05/26/logistic-regression/">机器学习之Logistic回归</a>
        </li>
        
        <li>
            <a href="/2017/05/20/linear-regression/">机器学习之线性回归</a>
        </li>
        
        <li>
            <a href="/2017/04/10/probability/">机器学习之概率论</a>
        </li>
        
        <li>
            <a href="/2017/04/03/math-analysis/">机器学习之数学分析</a>
        </li>
        
        <li>
            <a href="/2016/09/16/redis-lock/">基于redis的分布式锁的实现方案</a>
        </li>
        
    </ol>
</section>

    
    
        <section>
    
        
    
        
        <h4>Categories</h4>
        <p>
            
            <a class="badge badge-primary" href="/categories/%E5%B7%A5%E5%85%B7%E7%AE%B1">工具箱</a>
            
            <a class="badge badge-primary" href="/categories/%E6%8A%80%E6%9C%AF%E5%BF%97">技术志</a>
            
            <a class="badge badge-primary" href="/categories/%E6%9D%82%E8%B0%88%E9%9B%86">杂谈集</a>
            
        </p>
        
    
        
    
        
        <h4>Tags</h4>
        <p>
            
            <a class="badge badge-primary" href="/tags/apache">apache</a>
            
            <a class="badge badge-primary" href="/tags/golang">golang</a>
            
            <a class="badge badge-primary" href="/tags/iptables">iptables</a>
            
            <a class="badge badge-primary" href="/tags/linux">linux</a>
            
            <a class="badge badge-primary" href="/tags/logistic%E5%9B%9E%E5%BD%92">logistic回归</a>
            
            <a class="badge badge-primary" href="/tags/machine-learning">machine-learning</a>
            
            <a class="badge badge-primary" href="/tags/network">network</a>
            
            <a class="badge badge-primary" href="/tags/php">php</a>
            
            <a class="badge badge-primary" href="/tags/redis">redis</a>
            
            <a class="badge badge-primary" href="/tags/%E4%BB%A3%E7%90%86">代理</a>
            
            <a class="badge badge-primary" href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8">分布式存储</a>
            
            <a class="badge badge-primary" href="/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81">分布式锁</a>
            
            <a class="badge badge-primary" href="/tags/%E5%8E%8B%E5%8A%9B%E6%B5%8B%E8%AF%95">压力测试</a>
            
            <a class="badge badge-primary" href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%80%A7%E8%83%BD">服务器性能</a>
            
            <a class="badge badge-primary" href="/tags/%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%A8%A1%E5%9E%8B">服务器模型</a>
            
            <a class="badge badge-primary" href="/tags/%E6%AD%A3%E5%88%99%E5%8C%96">正则化</a>
            
            <a class="badge badge-primary" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92">线性回归</a>
            
        </p>
        
    
</section>
    
</aside>


      </div>
    </div>
    

    
      <footer class="blog-footer w-100">
    <nav class="navbar navbar-light bg-light">
        <p class="w-100 text-center">Hugo template made with ❤ by <a href="https://github.com/Xzya">Xzya</a>, inspired by <a href="https://github.com/alanorth/hugo-theme-bootstrap4-blog">hugo-theme-bootstrap4-blog</a></p>
        <p class="w-100 text-center"><a href="#">Back to top</a></p>
    </nav>
</footer>
    

    
    
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>
  </body>
</html>